# lightning 框架的配置文件
solver_conf:
    devices: [0]
    max_epochs: 200
    precision: "medium"
    strategy: 'ddp'  # dp
    accelerator: "gpu"
    accumulate_grad_batches: 4
    gradient_clip: 1
    train_batch_size: 1
    valid_batch_size: 1
    test_batch_size: 1
    num_workers: 1


save_conf:
    name: "stage_1"
    save_top_k: 20
    every_n_epochs: 1
    save_dir: "exp"

inference_conf:
    use_cuda: true
    inference_type: 'greedy_search'# 'greedy_search' 'beam_search' 对inference起作用, 目前beam search非并行，test会很慢
    beam_width: 3  # beam search 搜索宽度
    beam_nbest: 3  # beam search 保留结果，test阶段强制为1
    c_weight: 1  # 长度惩罚系数
    view_nbest: False  # inference 阶段展示nbest个结果




# args.yaml
output_dir: "/ssd/zhuang/code/wenet/examples/pt_deployment"
checkpoint: null
config: '/ssd/zhuang/code/wenet/examples/cn_correction/conf/train_transformer.yaml'
cv_data: "/ssd/zhuang/code/FunASR/examples/ChineseCorrection/data/paraformer_text/dev_lab.txt"
data_type: 'raw'
deepscale: False
deepscale_config: null
deepspeed: False
deepspeed_config: 'conf/ds_stage2.json'
device: 'cuda'
dist_backend: 'nccl'
dtype: 'fp32'
enc_init: null
enc_init_mods: ['audio_encoder.']
fp16_grad_sync: False
freeze_modules: []
fsdp_cpu_offload: False
fsdp_sharding_strategy: 'zero2'
fsdp_sync_module_states: True
jit: False
local_rank: -1
lora_alpha: 8
lora_attn_attr: ['self_attn', 'src_attn']
lora_ckpt_path: null
lora_dropout: 0
lora_init_yaml: 'wenet/finetune/lora/config.yaml'
lora_list: ['linear_out', 'linear_q', 'linear_k', 'linear_v']
lora_modules: ['audio_encoder.encoders']
lora_rank: 8
lora_reinit: False
model_dir: 'exp/paraformerV2'
num_workers: 8
only_optimize_lora: False
override_config: []
pin_memory: True
prefetch: 500
print_model: False
save_states: 'model_only'
tensorboard_dir: 'tensorboard'
test_data: "/ssd/zhuang/code/FunASR/examples/ChineseCorrection/data/paraformer_text/dev_lab.txt"
timeout: 30
train_data: "/ssd/zhuang/code/FunASR/examples/ChineseCorrection/data/paraformer_text/dev_lab.txt"
train_engine: 'torch_ddp'
use_amp: False
use_lora: False

